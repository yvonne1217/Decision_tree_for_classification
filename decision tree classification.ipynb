{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考\n",
    "\n",
    "[1] https://zhuanlan.zhihu.com/p/197476119\n",
    "\n",
    "[2] https://www.bilibili.com/video/BV1Mh411M7nV/?spm_id_from=333.337.search-card.all.click&vd_source=4e27374e03bfc58f9262019338fe1fa0\n",
    "\n",
    "[3]https://www.cnblogs.com/lsm-boke/p/12256686.html\n",
    "\n",
    "[4]https://www.knowledgedict.com/tutorial/sklearn-dataset.html\n",
    "\n",
    "[5]决策树2 基尼系数、基尼系数增益 - empirexu的文章 - 知乎\n",
    "https://zhuanlan.zhihu.com/p/456351465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None,pro_value=None):\n",
    "       \n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        self.value = value \n",
    "        self.pro_value=pro_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预剪枝决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2,max_depth=2,percent=10):\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.percent=percent\n",
    "        self.actual_tree_depth = 0  # 实际树深度\n",
    "\n",
    "    def build_tree(self, dataset, curr_depth=0, X_val=None, y_val=None):\n",
    "        X, Y = dataset[:, :-1], dataset[:, -1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        # 统计类别分布\n",
    "        class_counts = np.unique(Y, return_counts=True)\n",
    "        class_distribution = dict(zip(class_counts[0], class_counts[1]))\n",
    "        if num_samples >= self.min_samples_split and curr_depth <= self.max_depth:\n",
    "            # 检查类别分布条件\n",
    "            if len(class_distribution) > 1:  # 至少有两个类别\n",
    "                major_class_count = max(class_distribution.values())\n",
    "                minor_class_count = min(class_distribution.values())\n",
    "                if minor_class_count > 0 and major_class_count / minor_class_count > self.percent:\n",
    "                    print(f\"预剪枝：深度 {curr_depth}，类别分布 {class_distribution} -> 停止分裂\")\n",
    "                    leaf_value = self.calculate_leaf_value(Y)\n",
    "                    return Node(value=leaf_value)\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            if best_split and \"info_gain\" in best_split and best_split[\"info_gain\"] > 0:\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth + 1, X_val, y_val)\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth + 1, X_val, y_val)\n",
    "                print(f\"当前深度 {curr_depth}, 特征 {best_split['feature_index']}, 阈值 {best_split['threshold']:.2f}\")\n",
    "                print(f\"  样本数: {len(best_split['dataset'])}, 类别分布: {best_split['class_distribution']} \")\n",
    "                print(f\"  样本数最多的类: {best_split['value']}\")\n",
    "                print(f\"  左子树样本数: {len(best_split['dataset_left'])}, 类别分布: {best_split['left_class_distribution']} \")\n",
    "                print(f\"  右子树样本数: {len(best_split['dataset_right'])}, 类别分布: {best_split['right_class_distribution'] }\")\n",
    "                current_node = Node(best_split[\"feature_index\"], best_split[\"threshold\"],\n",
    "                                    left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "                current_node.left_class_distribution = best_split[\"left_class_distribution\"]\n",
    "                current_node.right_class_distribution = best_split[\"right_class_distribution\"]\n",
    "                current_node.class_distribution = best_split[\"class_distribution\"]\n",
    "                current_node.pro_value = best_split[\"value\"]\n",
    "                return current_node\n",
    "        # 若无法继续分裂，返回叶节点\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = np.sort(dataset[:, feature_index])[::-1]  \n",
    "            possible_thresholds = np.unique(feature_values)  \n",
    "            # 计算每相邻两个值的平均值，并将其添加到 possible_thresholds 中\n",
    "            average_thresholds = (feature_values[:-1] + feature_values[1:]) / 2  # 相邻元素的平均值\n",
    "            possible_thresholds = np.unique(np.concatenate((possible_thresholds, average_thresholds)))\n",
    "            # 合并并去重\n",
    "            for threshold in possible_thresholds:\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"else\")\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"dataset\"]=dataset\n",
    "                        classes=np.unique(best_split[\"dataset\"][:, -1], return_counts=True)\n",
    "                        best_split[\"class_distribution\"] = dict(zip(classes[0], classes[1]))\n",
    "                        majority_class = max(best_split[\"class_distribution\"], key=best_split[\"class_distribution\"].get)\n",
    "                        best_split[\"value\"] = majority_class\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        left_classes = np.unique(best_split[\"dataset_left\"][:, -1], return_counts=True)\n",
    "                        right_classes = np.unique(best_split[\"dataset_right\"][:, -1], return_counts=True)\n",
    "                        best_split[\"left_class_distribution\"] = dict(zip(left_classes[0], left_classes[1]))\n",
    "                        best_split[\"right_class_distribution\"] = dict(zip(right_classes[0], right_classes[1]))\n",
    "                        max_info_gain = curr_info_gain\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        # 左边是某个特征值小于阈值的样本，右边是某个特征值大于阈值的样本\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>=threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    # 计算信息增益\n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"gini\"):\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode=='gini':\n",
    "            info_gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) \n",
    "                                                   + weight_r*self.gini_index(r_child))\n",
    "        else:\n",
    "            info_gain = self.entropy(parent) - (weight_l*self.entropy(l_child) \n",
    "                                                + weight_r*self.entropy(r_child))\n",
    "        return info_gain\n",
    "    # 计算基尼系数\n",
    "    def gini_index(self, y):\n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / (len(y))\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "    # 计算熵\n",
    "    def entropy(self, y):\n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    def calculate_tree_depth(self, node):\n",
    "        \"\"\"\n",
    "        计算树的实际深度（递归遍历树）。\n",
    "        \"\"\"\n",
    "        if node is None or node.value is not None:  # 叶节点深度为 1\n",
    "            return 0\n",
    "        left_depth = self.calculate_tree_depth(node.left)\n",
    "        right_depth = self.calculate_tree_depth(node.right)\n",
    "        return 1 + max(left_depth, right_depth)\n",
    "    \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "   \n",
    "    def fit(self, X, Y, X_val=None, y_val=None):\n",
    "        if Y.ndim == 1:\n",
    "            Y = Y.reshape(-1, 1)  # 确保 Y 是二维数组\n",
    "        # 将数据拼接为一个整体\n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        # 构建决策树\n",
    "        self.root = self.build_tree(dataset, X_val=X_val, y_val=y_val)\n",
    "        self.actual_tree_depth = self.calculate_tree_depth(self.root)-1\n",
    "        print(f\"实际树深度: {self.actual_tree_depth}\")\n",
    "    def predict(self, X):\n",
    "        if self.root is None:\n",
    "            print(\"空树\")\n",
    "            return [None] * len(X)  \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        if tree is None:\n",
    "            raise ValueError(\"空树\")\n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "\n",
    "    def visualize_tree(self, tree=None, dot=None, node_id=0):\n",
    "        if dot is None:\n",
    "            dot = Digraph()\n",
    "            dot.node(name=str(node_id), label=\"Root\")  # 设置根节点的名称\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "        if tree.value is not None:\n",
    "            dot.node(name=str(node_id), label=\"Class: \" + str(tree.value))\n",
    "        else:\n",
    "            left_distribution = tree.left_class_distribution if tree.left_class_distribution else {}\n",
    "            right_distribution = tree.right_class_distribution if tree.right_class_distribution else {}\n",
    "            left_dist_str = \", \".join([f\"{int(key)}: {value}\" for key, value in left_distribution.items()])\n",
    "            right_dist_str = \", \".join([f\"{int(key)}: {value}\" for key, value in right_distribution.items()])\n",
    "            label = f\"X_{tree.feature_index} <= {tree.threshold:.2f}\\\\ninfo gain: {tree.info_gain:.2f}\"\n",
    "            label += f\"\\\\nLeft: {left_dist_str}\\\\nRight: {right_dist_str}\"\n",
    "            dot.node(name=str(node_id), label=label)\n",
    "            left_child_id = node_id * 2 + 1\n",
    "            dot.edge(str(node_id), str(left_child_id), label=\"True\")  # 左分支\n",
    "            self.visualize_tree(tree.left, dot, left_child_id)\n",
    "            right_child_id = node_id * 2 + 2\n",
    "            dot.edge(str(node_id), str(right_child_id), label=\"False\")  # 右分支\n",
    "            self.visualize_tree(tree.right, dot, right_child_id)\n",
    "        return dot\n",
    "    \n",
    "    def prune(self, node, X_val, y_val,curr_depth=0,tree_depth=None):\n",
    "        if node.left is None and node.right is None: return\n",
    "        if node.left:\n",
    "            self.prune(node.left, X_val, y_val,curr_depth+1,tree_depth)\n",
    "        if node.right:\n",
    "            self.prune(node.right, X_val, y_val,curr_depth+1,tree_depth)\n",
    "        if curr_depth == tree_depth :\n",
    "            be_predictions = self.predict(X_val)\n",
    "            left = node.left\n",
    "            right = node.right\n",
    "            node.left = None\n",
    "            node.right = None\n",
    "            node.value = node.pro_value\n",
    "            predictions = self.predict(X_val)\n",
    "            y_val = np.array(y_val).ravel()  # 确保 y_val 是一维数组\n",
    "            predictions = np.array(predictions).ravel()  # 确保预测结果也是一维数组\n",
    "            label_encoder = LabelEncoder()\n",
    "            y_val_encoded = label_encoder.fit_transform(y_val)\n",
    "            be_predictions_encoded = label_encoder.transform(be_predictions)\n",
    "            predictions_encoded = label_encoder.transform(predictions)\n",
    "            acc_pruned = accuracy_score(y_val_encoded, predictions_encoded)\n",
    "            acc_unpruned = accuracy_score(y_val_encoded, be_predictions_encoded)  # 保留未剪枝节点的准确性\n",
    "            print(f\"剪枝特征: {node.feature_index}, 剪枝前: {acc_unpruned}, 剪枝后: {acc_pruned}\")\n",
    "            if acc_pruned <= acc_unpruned:\n",
    "                node.left = left\n",
    "                node.right = right\n",
    "                node.value = None\n",
    "            else:\n",
    "                print(f\"特征 {node.feature_index} 剪枝后性能提高。\")\n",
    "        \n",
    "    def fit_with_pruning(self, X, Y, X_val, y_val):\n",
    "        self.fit(X, Y)\n",
    "        for i in range(0,self.actual_tree_depth):\n",
    "             self.prune(self.root, X_val, y_val, curr_depth=0,\n",
    "                         tree_depth=self.actual_tree_depth-i)\n",
    "\n",
    "        # self.prune(self.root, X_val, y_val, curr_depth=0, tree_depth=self.actual_tree_depth)\n",
    "        # self.prune(self.root, X_val, y_val, curr_depth=0, tree_depth=self.actual_tree_depth-1)\n",
    "        # self.prune(self.root, X_val, y_val, curr_depth=0, tree_depth=self.actual_tree_depth-2)\n",
    "        # self.prune(self.root, X_val, y_val, curr_depth=0, tree_depth=self.actual_tree_depth-3)\n",
    "        # self.prune(self.root, X_val, y_val, curr_depth=0, tree_depth=self.actual_tree_depth-4)\n",
    "        # self.prune(self.root, X_val, y_val, curr_depth=0, tree_depth=self.actual_tree_depth-5)\n",
    "        # 然后进行剪枝\n",
    "        # for i in range(int(self.actual_tree_depth),1):\n",
    "        #     self.prune(self.root, X_val, y_val, curr_depth=0,tree_depth=i)\n",
    "        #     print(f\"i: {i}\")\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从文件导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征数据 X:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        15.66         23.20          110.20      773.5          0.11090   \n",
      "1        15.46         23.95          103.80      731.3          0.11830   \n",
      "2        11.47         16.03           73.02      402.7          0.09076   \n",
      "3        19.55         28.77          133.60     1207.0          0.09260   \n",
      "4        12.03         17.93           76.09      446.0          0.07683   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.31140        0.317600             0.137700         0.2495   \n",
      "1           0.18700        0.203000             0.085200         0.1807   \n",
      "2           0.05886        0.025870             0.023220         0.1634   \n",
      "3           0.20630        0.178400             0.114400         0.1893   \n",
      "4           0.03892        0.001546             0.005592         0.1382   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.08104  ...         19.85          31.64           143.70   \n",
      "1                 0.07083  ...         17.11          36.33           117.70   \n",
      "2                 0.06372  ...         12.51          20.79            79.67   \n",
      "3                 0.06232  ...         25.05          36.27           178.60   \n",
      "4                 0.06070  ...         13.07          22.25            82.74   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      1226.0            0.1504             0.5172         0.618100   \n",
      "1       909.4            0.1732             0.4967         0.591100   \n",
      "2       475.8            0.1531             0.1120         0.098230   \n",
      "3      1926.0            0.1281             0.5329         0.425100   \n",
      "4       523.4            0.1013             0.0739         0.007732   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0               0.24620          0.3277                  0.10190  \n",
      "1               0.21630          0.3013                  0.10670  \n",
      "2               0.06548          0.2851                  0.08763  \n",
      "3               0.19410          0.2818                  0.10050  \n",
      "4               0.02796          0.2171                  0.07037  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "标签数据 y:\n",
      "   target\n",
      "0       0\n",
      "1       0\n",
      "2       1\n",
      "3       0\n",
      "4       1\n",
      "[[  15.66   23.2   110.2   773.5 ]\n",
      " [  15.46   23.95  103.8   731.3 ]\n",
      " [  11.47   16.03   73.02  402.7 ]\n",
      " [  19.55   28.77  133.6  1207.  ]\n",
      " [  12.03   17.93   76.09  446.  ]]\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: 从 CSV 文件读取数据\n",
    "file1_path = './dataset_1/X_train.csv'  # 替换为特征数据的 CSV 文件路径\n",
    "file2_path = './dataset_1/y_train.csv'  # 替换为标签数据的 CSV 文件路径\n",
    "file3_path = './dataset_1/X_test.csv'  # 替换为特征数据的 CSV 文件路径\n",
    "file4_path = './dataset_1/y_test.csv'  # 替换为标签数据的 CSV 文件路径\n",
    "# 使用 pandas 读取 CSV 文件\n",
    "X_train = pd.read_csv(file1_path)  # 假设文件包含标题行\n",
    "y_train = pd.read_csv(file2_path)  # 假设文件包含标题行\n",
    "X_test = pd.read_csv(file3_path)  # 假设文件包含标题行\n",
    "y_test = pd.read_csv(file4_path)  # 假设文件包含标题行\n",
    "print(\"特征数据 X:\")\n",
    "print(X_train.head())\n",
    "print(\"\\n标签数据 y:\")\n",
    "print(y_train.head())\n",
    "X_train=X_train.iloc[:, :4].values\n",
    "print(X_train[:5])\n",
    "X_test=X_test.iloc[:, :4].values\n",
    "\n",
    "y_train = y_train.iloc[:, 0].values.reshape(-1,1)\n",
    "print(y_train[:5])\n",
    "y_test = y_test.iloc[:, 0].values.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前深度 5, 特征 1, 阈值 15.58\n",
      "  样本数: 106, 类别分布: {0.0: 9, 1.0: 97} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 1, 类别分布: {0.0: 1} \n",
      "  右子树样本数: 105, 类别分布: {0.0: 8, 1.0: 97}\n",
      "当前深度 4, 特征 0, 阈值 11.19\n",
      "  样本数: 107, 类别分布: {0.0: 10, 1.0: 97} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 1, 类别分布: {0.0: 1} \n",
      "  右子树样本数: 106, 类别分布: {0.0: 9, 1.0: 97}\n",
      "当前深度 3, 特征 2, 阈值 73.29\n",
      "  样本数: 147, 类别分布: {0.0: 10, 1.0: 137} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 40, 类别分布: {1.0: 40} \n",
      "  右子树样本数: 107, 类别分布: {0.0: 10, 1.0: 97}\n",
      "当前深度 2, 特征 1, 阈值 15.54\n",
      "  样本数: 219, 类别分布: {0.0: 10, 1.0: 209} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 72, 类别分布: {1.0: 72} \n",
      "  右子树样本数: 147, 类别分布: {0.0: 10, 1.0: 137}\n",
      "当前深度 5, 特征 2, 阈值 77.40\n",
      "  样本数: 16, 类别分布: {0.0: 1, 1.0: 15} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 13, 类别分布: {1.0: 13} \n",
      "  右子树样本数: 3, 类别分布: {0.0: 1, 1.0: 2}\n",
      "当前深度 4, 特征 1, 阈值 21.30\n",
      "  样本数: 17, 类别分布: {0.0: 2, 1.0: 15} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 16, 类别分布: {0.0: 1, 1.0: 15} \n",
      "  右子树样本数: 1, 类别分布: {0.0: 1}\n",
      "当前深度 3, 特征 1, 阈值 21.38\n",
      "  样本数: 47, 类别分布: {0.0: 2, 1.0: 45} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 17, 类别分布: {0.0: 2, 1.0: 15} \n",
      "  右子树样本数: 30, 类别分布: {1.0: 30}\n",
      "当前深度 5, 特征 3, 阈值 506.85\n",
      "  样本数: 22, 类别分布: {0.0: 7, 1.0: 15} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 7, 类别分布: {0.0: 5, 1.0: 2} \n",
      "  右子树样本数: 15, 类别分布: {0.0: 2, 1.0: 13}\n",
      "当前深度 5, 特征 3, 阈值 589.45\n",
      "  样本数: 31, 类别分布: {0.0: 19, 1.0: 12} \n",
      "  样本数最多的类: 0.0\n",
      "  左子树样本数: 7, 类别分布: {0.0: 7} \n",
      "  右子树样本数: 24, 类别分布: {0.0: 12, 1.0: 12}\n",
      "当前深度 4, 特征 2, 阈值 87.34\n",
      "  样本数: 53, 类别分布: {0.0: 26, 1.0: 27} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 22, 类别分布: {0.0: 7, 1.0: 15} \n",
      "  右子树样本数: 31, 类别分布: {0.0: 19, 1.0: 12}\n",
      "当前深度 3, 特征 1, 阈值 27.70\n",
      "  样本数: 58, 类别分布: {0.0: 26, 1.0: 32} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 53, 类别分布: {0.0: 26, 1.0: 27} \n",
      "  右子树样本数: 5, 类别分布: {1.0: 5}\n",
      "当前深度 2, 特征 2, 阈值 80.79\n",
      "  样本数: 105, 类别分布: {0.0: 28, 1.0: 77} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 47, 类别分布: {0.0: 2, 1.0: 45} \n",
      "  右子树样本数: 58, 类别分布: {0.0: 26, 1.0: 32}\n",
      "当前深度 1, 特征 1, 阈值 19.62\n",
      "  样本数: 324, 类别分布: {0.0: 38, 1.0: 286} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 219, 类别分布: {0.0: 10, 1.0: 209} \n",
      "  右子树样本数: 105, 类别分布: {0.0: 28, 1.0: 77}\n",
      "当前深度 4, 特征 1, 阈值 14.09\n",
      "  样本数: 6, 类别分布: {0.0: 3, 1.0: 3} \n",
      "  样本数最多的类: 0.0\n",
      "  左子树样本数: 4, 类别分布: {0.0: 1, 1.0: 3} \n",
      "  右子树样本数: 2, 类别分布: {0.0: 2}\n",
      "当前深度 3, 特征 0, 阈值 16.09\n",
      "  样本数: 10, 类别分布: {0.0: 3, 1.0: 7} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 6, 类别分布: {0.0: 3, 1.0: 3} \n",
      "  右子树样本数: 4, 类别分布: {1.0: 4}\n",
      "当前深度 2, 特征 2, 阈值 114.80\n",
      "  样本数: 13, 类别分布: {0.0: 6, 1.0: 7} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 10, 类别分布: {0.0: 3, 1.0: 7} \n",
      "  右子树样本数: 3, 类别分布: {0.0: 3}\n",
      "预剪枝：深度 2，类别分布 {0.0: 117, 1.0: 1} -> 停止分裂\n",
      "当前深度 1, 特征 1, 阈值 16.38\n",
      "  样本数: 131, 类别分布: {0.0: 123, 1.0: 8} \n",
      "  样本数最多的类: 0.0\n",
      "  左子树样本数: 13, 类别分布: {0.0: 6, 1.0: 7} \n",
      "  右子树样本数: 118, 类别分布: {0.0: 117, 1.0: 1}\n",
      "当前深度 0, 特征 3, 阈值 697.80\n",
      "  样本数: 455, 类别分布: {0.0: 161, 1.0: 294} \n",
      "  样本数最多的类: 1.0\n",
      "  左子树样本数: 324, 类别分布: {0.0: 38, 1.0: 286} \n",
      "  右子树样本数: 131, 类别分布: {0.0: 123, 1.0: 8}\n",
      "实际树深度: 5\n",
      "剪枝特征: 1, 剪枝前: 0.8859649122807017, 剪枝后: 0.8859649122807017\n",
      "剪枝特征: 2, 剪枝前: 0.8859649122807017, 剪枝后: 0.8859649122807017\n",
      "剪枝特征: 3, 剪枝前: 0.8859649122807017, 剪枝后: 0.8859649122807017\n",
      "剪枝特征: 3, 剪枝前: 0.8859649122807017, 剪枝后: 0.9298245614035088\n",
      "特征 3 剪枝后性能提高。\n",
      "剪枝特征: 0, 剪枝前: 0.9298245614035088, 剪枝后: 0.9298245614035088\n",
      "剪枝特征: 1, 剪枝前: 0.9298245614035088, 剪枝后: 0.9298245614035088\n",
      "剪枝特征: 2, 剪枝前: 0.9298245614035088, 剪枝后: 0.8859649122807017\n",
      "剪枝特征: 1, 剪枝前: 0.9298245614035088, 剪枝后: 0.9210526315789473\n",
      "剪枝特征: 2, 剪枝前: 0.9298245614035088, 剪枝后: 0.9298245614035088\n",
      "剪枝特征: 1, 剪枝前: 0.9298245614035088, 剪枝后: 0.9298245614035088\n",
      "剪枝特征: 1, 剪枝前: 0.9298245614035088, 剪枝后: 0.8859649122807017\n",
      "剪枝特征: 0, 剪枝前: 0.9298245614035088, 剪枝后: 0.9298245614035088\n",
      "剪枝特征: 1, 剪枝前: 0.9298245614035088, 剪枝后: 0.9298245614035088\n",
      "剪枝特征: 2, 剪枝前: 0.9298245614035088, 剪枝后: 0.8859649122807017\n",
      "剪枝特征: 2, 剪枝前: 0.9298245614035088, 剪枝后: 0.9035087719298246\n",
      "剪枝特征: 1, 剪枝前: 0.9298245614035088, 剪枝后: 0.8859649122807017\n",
      "剪枝特征: 1, 剪枝前: 0.9298245614035088, 剪枝后: 0.9210526315789473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'decision_tree_visualize_cp.png'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step1:导入数据\n",
    "from sklearn import datasets\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X_c = cancer.data # 特征数据\n",
    "y_c = cancer.target # 标签数据\n",
    "y_c_re=y_c[:,np.newaxis]\n",
    "\n",
    "# Step2:划分数据集,后缀为0的是由sklearn库里面得到的数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train0, X_test0, y_train0, y_test0 = train_test_split(X_c, y_c, test_size=0.2, random_state=33)\n",
    "\n",
    "# Step3:得到决策树\n",
    "classifier_c = DecisionTreeClassifier(min_samples_split=5, max_depth=5,percent=50)\n",
    "# classifier_c.fit(X_train,y_train)\n",
    "# classifier_c.fit(X_train0,y_train0)\n",
    "\n",
    "# Step4:决策树可视化\n",
    "# dot_c = classifier_c.visualize_tree()\n",
    "# dot_c.render(\"decision_tree_visualize_c\", format=\"png\")  # 保存为 PNG 文件\n",
    "\n",
    "# Step5:准确率\n",
    "# y_pred = classifier_c.predict(X_test) \n",
    "# accuracy_score(y_test, y_pred)\n",
    "# y_pred0 = classifier_c.predict(X_test0) \n",
    "# accuracy_score(y_test0, y_pred0)\n",
    "\n",
    "# 后剪枝\n",
    "classifier_c.fit_with_pruning(X_train, y_train,X_test, y_test)\n",
    "dot_cp = classifier_c.visualize_tree()\n",
    "dot_cp.render(\"decision_tree_visualize_cp\", format=\"png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
